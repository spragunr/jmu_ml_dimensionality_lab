{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Curse of Dimensionality\n",
    "\n",
    "Supervised learning is, in essense, about *generalization*.  We fit models to training data, and we expect those models to make accurate\n",
    "predictions on unseen data.  This is only possible if our training\n",
    "data is, in some sense, similar to our testing data.  This raises some\n",
    "important questions:\n",
    "\n",
    "1) What exactly do we mean by similarity (or dissimilarity)?\n",
    "\n",
    "2) How much data do we need?  If we hope to make an accurate\n",
    "prediction about a testing point, we clearly need enough training data\n",
    "to be confident that we have seen at least one example that is similar\n",
    "to it.  How much is that?\n",
    "\n",
    "We'll get back to question 1).  Question 2) is related to the\n",
    "*dimensionality* of our data set.\n",
    "\n",
    "## Question\n",
    "\n",
    "*   Imagine we have divided our data space into grid cells.  We are willing to generalize from old data as long as we have at least *one* training point in the same grid cell as our testing point.  Our granularity will be 10 cells per dimension.  So if we are making predictions from 1d data, our input space is divided up like this:\n",
    "\n",
    "    ![1d grid](images/1d_grid.png)\n",
    "\n",
    "    If we are making predictions from 2d data, our input space is divided up like this:\n",
    "\n",
    "    ![2d grid](images/2d_grid.png)\n",
    "\n",
    "    How much data do we need for one-dimensional data?  How much for two-dimensional data?  How much for 100-dimensional data?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Another Way Of Looking At The Problem\n",
    "\n",
    "The cell below generates some random data points and plots a histogram of the distances between them. Try changing the dimensionality of the data and looking at how this changes the distance distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib qt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics.pairwise import euclidean_distances\n",
    "\n",
    "samples = 1000\n",
    "dims = 2\n",
    "\n",
    "points = np.random.rand(samples, dims)\n",
    "dist = euclidean_distances(points) # get distances between all points\n",
    "\n",
    "upper_tri = dist[np.triu_indices(samples,k=1)]\n",
    "upper_tri_mu = np.mean(upper_tri)\n",
    "upper_tri_std = np.std(upper_tri)\n",
    "\n",
    "n, bins, patches = plt.hist(x=upper_tri, bins='auto', color='blue',\n",
    "                            alpha=0.7)\n",
    "plt.grid(axis='y', alpha=0.75)\n",
    "plt.xlim(0, max(bins))\n",
    "plt.xlabel('Distance',fontsize=18)\n",
    "plt.ylabel('Count',fontsize=18)\n",
    "plt.title(str(samples) + ' Points in ' + str(dims) +\n",
    "          ' dim space ' + r'$\\mu$=%0.1f std=%0.1f'%\n",
    "          (upper_tri_mu,upper_tri_std),fontsize=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "* How does the number of randomly generated points that are near each other change as the dimensionality increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# There is Hope!\n",
    "\n",
    "High-dimensional data is a fundamental challenge in machine learning.  For example, consider image data.  Each data item is composed of many thousands of individual attributes representing the color values for each pixel.  How can we hope to work with this sort of data given the issues illustrated above???"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source of Hope \\# 1 - Irrelevant (or less relevant) Attributes\n",
    "\n",
    "Sometimes, not all of the attributes in the dataset are actually relevent to the learning problem we are trying to solve.  For example, our goal for the data set below is to predict the value of the final column using the other five attributes.  Take a look at this subset of the data to see if you can see the relationship:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.12433  0.67385  1.      -0.36333  0.89254 -0.24483]\n",
      " [-0.14731  0.58515  0.      -1.00506  0.71352 -0.58812]\n",
      " [-0.99205  0.93073  0.       0.95289  0.32335  0.88688]\n",
      " [-0.59885  0.77542  1.      -2.2056   0.17221 -1.71027]\n",
      " [-1.81108  0.96133  0.5     -1.71702  0.59198 -1.65063]\n",
      " [-1.14077  0.19882  0.       0.78885  0.4011   0.15684]\n",
      " [ 0.98275  0.73843  0.       0.14252  0.71794  0.10524]\n",
      " [-0.5651   0.32222  0.       1.79837  0.52331  0.57947]\n",
      " [ 0.5575   0.96172  1.       0.41839  0.97731  0.40238]\n",
      " [ 1.30389  0.89171  0.5     -0.21014  0.71333 -0.18738]]\n"
     ]
    }
   ],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "data = np.loadtxt('data/dims.csv', delimiter=',')\n",
    "print(data[0:10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Assuming it isn't jumping out at you, we can do some exploratory data analysys by plotting our target value against each of the different attributes.  See if you can figure out if any of the attributes appear relevant.  If you find two relevant attributes, you can try plotting a 3d scatter plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the target value against attribute 0:\n",
    "plt.plot(data[:, 0], data[:, 5], '*')\n",
    "plt.show()\n",
    "\n",
    "# Uncomment the code below to create a 3d scatter plot...\n",
    "\n",
    "# Plot the target value against attribute 0:\n",
    "\n",
    "#fig = plt.figure()\n",
    "#ax = fig.add_subplot(111, projection='3d')\n",
    "#ax.scatter(data[:,0], data[:, 1], data[:, 5])\n",
    "#plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "*   Which attributes are most relevant?\n",
    "*   What what output value would you predict for the attributes\n",
    "    [ 0.719  0.304  0.          0.328  0.124].\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source of Hope \\# 2 - Lower Dimensional Embeddings\n",
    "\n",
    "Sometimes,  what *appears* to be high-dimensional data is actually low-dimensional data embedded in a high dimensional space. Try creating a scatter plot of the following three-dimensional data set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.04376 0.75309 0.89138]\n",
      " [0.82159 0.99307 0.20919]\n",
      " [0.50953 0.29039 0.53016]\n",
      " [0.61913 0.20959 0.82045]\n",
      " [0.51758 0.4721  0.32064]\n",
      " [0.74791 0.86306 0.24082]\n",
      " [0.86459 0.39199 1.02356]\n",
      " [0.86223 0.93845 0.34716]\n",
      " [0.79643 0.62851 0.61387]\n",
      " [1.01863 0.87677 0.69545]]\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('data/embed.csv', delimiter=',')\n",
    "print(data[0:10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "* How would you describe the \"true\" dimensionality of this data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Source of Hope \\# 3 - Manifolds\n",
    "\n",
    "Sometimes, what appears to be high-demensional data is actually low-dimensional data embedded in a high-dimensional *manifold*.  Inspect the data below as a 3d scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.79218  0.71804  0.64258]\n",
      " [ 0.40085  0.65681  0.23334]\n",
      " [ 0.11685  0.50176  0.55479]\n",
      " [ 0.64464  0.77364  0.34144]\n",
      " [ 1.19689  0.67484  0.71785]\n",
      " [ 0.08704  0.23069 -0.06788]\n",
      " [-0.01118  0.63169 -0.05429]\n",
      " [ 0.24954  0.59989  0.39266]\n",
      " [ 0.12     0.51133  0.4722 ]\n",
      " [ 1.2029   0.70008  0.80371]]\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('data/manifold.csv', delimiter=',')\n",
    "print(data[0:10, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question\n",
    "* How would you describe the \"true\" dimensionality of this data?\n",
    "* You were able to recognize that this data lies on a lower dimensional manifold by visually inspecting the 3d points. How would you reach this conclusion if the manifold was embedded in a 20-dimensional space?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
